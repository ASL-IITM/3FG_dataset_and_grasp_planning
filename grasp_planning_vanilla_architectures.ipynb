{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYvVcI8i725RbbKuGAxGPF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BijoSebastian/grasp_it_repo/blob/main/grasp_planning_vanilla_architectures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Applying Vanilla CNN, VGG-16, ResNet, InceptionNet(ViT if possible)\n",
        "1. Dataloader:\n",
        "*   Add all four channels of image into 1 RGBD image.\n",
        "*   Get position and orientation from csv file and try to add it in one dataloader or figure out some other way.\n",
        "\n",
        "2. Currently predicting only position and orientation of the gripper:\n",
        "*  Loss function for orientation to be quaternions is custom made.\n",
        "*  Total loss= (loss1)X(0.5) + (loss2)X(0.5)"
      ],
      "metadata": {
        "id": "dwQjsF3USsDh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyM2W-N9R7F2"
      },
      "outputs": [],
      "source": [
        "#DataLoader\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define custom dataset class\n",
        "class RGBDDataset(Dataset):\n",
        "    def __init__(self, rgb_dir, depth_dir, labels_file, transform=None):\n",
        "        self.rgb_dir = rgb_dir\n",
        "        self.depth_dir = depth_dir\n",
        "        self.labels_file = labels_file\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read labels from file (assuming labels are stored in a text file)\n",
        "        with open(labels_file, 'r') as f:\n",
        "            self.labels = [line.strip().split(',') for line in f.readlines()]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rgb_path = os.path.join(self.rgb_dir, self.labels[idx][0])  # Assuming filenames are in the first column\n",
        "        depth_path = os.path.join(self.depth_dir, self.labels[idx][1])  # Assuming depth image filenames are in the second column\n",
        "        # Load RGB and depth images\n",
        "        rgb_img = Image.open(rgb_path).convert('RGB')\n",
        "        depth_img = Image.open(depth_path).convert('L')  # Convert to grayscale\n",
        "\n",
        "        # Apply transformations if specified\n",
        "        if self.transform:\n",
        "            rgb_img = self.transform(rgb_img)\n",
        "            depth_img = self.transform(depth_img)\n",
        "\n",
        "        # Combine RGB and depth images into RGBD\n",
        "        rgbd_img = torch.cat((rgb_img, depth_img), dim=0)\n",
        "\n",
        "        # Extract labels (position and orientation)\n",
        "        position_label = np.array([float(x) for x in self.labels[idx][2:5]])  # Assuming position labels are in columns 2, 3, and 4\n",
        "        orientation_label = np.array([float(x) for x in self.labels[idx][5:]])  # Assuming orientation labels are in columns 5, 6, 7, and 8\n",
        "\n",
        "        return rgbd_img, position_label, orientation_label\n",
        "\n",
        "# Example usage:\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to match input size of CNN\n",
        "    transforms.ToTensor(),  # Convert images to tensors\n",
        "])\n",
        "\n",
        "# Initialize dataset\n",
        "rgb_dir = 'path_to_rgb_images_directory'\n",
        "depth_dir = 'path_to_depth_images_directory'\n",
        "labels_file = 'path_to_labels_file.txt'\n",
        "dataset = RGBDDataset(rgb_dir, depth_dir, labels_file, transform=transform)\n",
        "\n",
        "# Initialize DataLoader\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Assuming you have defined the CNN architecture as CNN_Position_Orientation\n",
        "model = CNN_Position_Orientation()\n",
        "\n",
        "# Assuming you have defined loss functions and optimizer\n",
        "position_criterion = nn.MSELoss()  # Mean Squared Error for position\n",
        "orientation_criterion = quaternion_loss  # Defined earlier\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (rgbd_images, position_labels, orientation_labels) in enumerate(dataloader):\n",
        "        # Forward pass\n",
        "        positions, orientations = model(rgbd_images)\n",
        "\n",
        "        # Calculate losses\n",
        "        position_loss = position_criterion(positions, position_labels)\n",
        "        orientation_loss = orientation_criterion(orientations, orientation_labels)\n",
        "        total_loss = position_loss + orientation_loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BFiIYUt7Sywn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper functions and Quaternion loss function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def quaternion_loss(q_actual, q_predicted):\n",
        "    \"\"\"\n",
        "    Compute quaternion loss between actual and predicted quaternions.\n",
        "\n",
        "    Args:\n",
        "        q_actual (torch.Tensor): Actual quaternions (batch_size x num_outputs x 4).\n",
        "        q_predicted (torch.Tensor): Predicted quaternions (batch_size x num_outputs x 4).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Quaternion loss.\n",
        "    \"\"\"\n",
        "    # Compute dot product\n",
        "    dot_product = torch.sum(q_actual * q_predicted, dim=2)\n",
        "\n",
        "    # Take absolute difference from 1\n",
        "    loss = 1 - torch.abs(dot_product)\n",
        "\n",
        "    return loss.mean()  # Return mean loss over the batch and number of outputs\n",
        "\n",
        "# Example usage:\n",
        "# Assuming positions and orientations are tensors of shape (batch_size, num_outputs, 3) and (batch_size, num_outputs, 4) respectively\n",
        "position_criterion = nn.MSELoss()  # Mean Squared Error for position\n",
        "\n",
        "# Calculate losses\n",
        "position_loss = position_criterion(positions, actual_positions)\n",
        "orientation_loss = quaternion_loss(orientations, actual_orientations)\n",
        "\n",
        "# Total loss\n",
        "total_loss = position_loss + orientation_loss\n",
        "\n",
        "# Backward pass and optimization\n",
        "optimizer.zero_grad()\n",
        "total_loss.backward()\n",
        "optimizer.step()\n"
      ],
      "metadata": {
        "id": "_GOFLHaSS0qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BXNgvuwgS2Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Vanilla CNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_Position_Orientation(nn.Module):\n",
        "    def __init__(self, num_outputs=10):\n",
        "        super(CNN_Position_Orientation, self).__init__()\n",
        "        self.num_outputs = num_outputs\n",
        "\n",
        "        # Define convolutional layers for image processing\n",
        "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Define fully connected layers for position prediction\n",
        "        self.fc_position1 = nn.Linear(64 * 32 * 32, 512)\n",
        "        self.fc_position2 = nn.Linear(512, 3 * num_outputs)  # Output 10 sets of 3D positions\n",
        "\n",
        "        # Define fully connected layers for orientation prediction\n",
        "        self.fc_orientation1 = nn.Linear(64 * 32 * 32, 512)\n",
        "        self.fc_orientation2 = nn.Linear(512, 4 * num_outputs)  # Output 10 sets of quaternions\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through convolutional layers\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "\n",
        "        # Flatten the output for fully connected layers\n",
        "        x = x.view(-1, 64 * 32 * 32)\n",
        "\n",
        "        # Position prediction\n",
        "        position = F.relu(self.fc_position1(x))\n",
        "        position = self.fc_position2(position)\n",
        "        position = position.view(-1, self.num_outputs, 3)  # Reshape to (batch_size, num_outputs, 3)\n",
        "\n",
        "        # Orientation prediction\n",
        "        orientation = F.relu(self.fc_orientation1(x))\n",
        "        orientation = self.fc_orientation2(orientation)\n",
        "        orientation = orientation.view(-1, self.num_outputs, 4)  # Reshape to (batch_size, num_outputs, 4)\n",
        "\n",
        "        return position, orientation\n",
        "\n",
        "# Example usage:\n",
        "# Instantiate the model\n",
        "model = CNN_Position_Orientation(num_outputs=10)\n",
        "\n",
        "# Assuming rgb_d_image is your input RGBD image tensor of shape (batch_size, channels, height, width)\n",
        "# Forward pass\n",
        "positions, orientations = model(rgb_d_image)\n",
        "\n",
        "# positions shape: (batch_size, num_outputs, 3)\n",
        "# orientations shape: (batch_size, num_outputs, 4)\n"
      ],
      "metadata": {
        "id": "pPKe-z1WSzKA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}